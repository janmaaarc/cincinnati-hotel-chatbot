{
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "hotel-chat",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook-node",
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [-600, 0],
      "webhookId": "hotel-chat-webhook"
    },
    {
      "parameters": {
        "promptType": "define",
        "messages": {
          "values": [
            {
              "type": "system",
              "message": "=You are Cincinnati Hotel's friendly virtual concierge. You help guests with questions about the hotel.\n\nHOTEL KNOWLEDGE BASE:\n{{ $json.body?.pdfContent || $json.pdfContent || 'No hotel information available.' }}\n\nINSTRUCTIONS:\n1. Search the HOTEL KNOWLEDGE BASE above carefully for the answer\n2. If you find the answer in the knowledge base, respond helpfully and set answerFound to true\n3. If you cannot find the answer, set answerFound to false and politely say you don't have that information\n4. Be warm, professional, and concise\n5. Always respond with valid JSON only\n\nYou MUST respond with ONLY valid JSON in this exact format (no markdown, no extra text):\n{\"answer\": \"your helpful response here\", \"category\": \"CategoryName\", \"answerFound\": true}\n\nValid categories: Rooms, Restaurant, Amenities, Location, Pricing, Policies, Services, Other"
            },
            {
              "type": "human",
              "message": "={{ $json.body?.message || $json.message }}"
            }
          ]
        }
      },
      "id": "llm-chain-node",
      "name": "Basic LLM Chain",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.4,
      "position": [-350, 0]
    },
    {
      "parameters": {
        "jsCode": "const response = $input.first().json;\n\nlet parsedResponse;\ntry {\n  const aiOutput = response.text || response.output || response.response || JSON.stringify(response);\n  \n  // Try to extract JSON from the response\n  const jsonMatch = aiOutput.match(/\\{[\\s\\S]*?\\}/);\n  if (jsonMatch) {\n    parsedResponse = JSON.parse(jsonMatch[0]);\n  } else {\n    parsedResponse = {\n      answer: String(aiOutput).substring(0, 500) || \"I apologize, I couldn't process your request.\",\n      category: 'Other',\n      answerFound: false\n    };\n  }\n} catch (error) {\n  parsedResponse = {\n    answer: \"I apologize, I'm having trouble processing your request right now.\",\n    category: 'Other',\n    answerFound: false\n  };\n}\n\nreturn {\n  answer: parsedResponse.answer || \"I couldn't find an answer.\",\n  category: parsedResponse.category || 'Other',\n  answerFound: parsedResponse.answerFound === true\n};"
      },
      "id": "parse-response-node",
      "name": "Parse Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-100, 0]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "id": "respond-node",
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [150, 0]
    },
    {
      "parameters": {
        "modelName": "models/gemini-1.5-flash",
        "options": {
          "temperature": 0.2
        }
      },
      "id": "gemini-model-node",
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [-350, 200],
      "credentials": {
        "googlePalmApi": {
          "id": "YOUR_CREDENTIAL_ID",
          "name": "Google Gemini API"
        }
      }
    }
  ],
  "connections": {
    "Webhook": {
      "main": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain": {
      "main": [
        [
          {
            "node": "Parse Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Response": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    }
  },
  "pinData": {},
  "meta": {
    "instanceId": "hotel-chatbot-instance"
  }
}
